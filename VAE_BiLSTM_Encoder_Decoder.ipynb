{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz-cIAAnSxKr",
        "outputId": "ac69bf02-bb92-4929-9ea1-b350ee059a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 169ms/step - loss: 0.0511 - val_loss: 0.0124 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 127ms/step - loss: 0.0157 - val_loss: 0.0118 - learning_rate: 9.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 144ms/step - loss: 0.0142 - val_loss: 0.0111 - learning_rate: 8.1000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 148ms/step - loss: 0.0141 - val_loss: 0.0112 - learning_rate: 7.2900e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 157ms/step - loss: 0.0135 - val_loss: 0.0108 - learning_rate: 6.5610e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 157ms/step - loss: 0.0138 - val_loss: 0.0108 - learning_rate: 5.9049e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 146ms/step - loss: 0.0135 - val_loss: 0.0104 - learning_rate: 5.3144e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 148ms/step - loss: 0.0136 - val_loss: 0.0107 - learning_rate: 4.7830e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 149ms/step - loss: 0.0137 - val_loss: 0.0107 - learning_rate: 4.3047e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 147ms/step - loss: 0.0141 - val_loss: 0.0103 - learning_rate: 3.8742e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 149ms/step - loss: 0.0134 - val_loss: 0.0104 - learning_rate: 3.4868e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 136ms/step - loss: 0.0130 - val_loss: 0.0105 - learning_rate: 3.1381e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 147ms/step - loss: 0.0132 - val_loss: 0.0103 - learning_rate: 2.8243e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 151ms/step - loss: 0.0136 - val_loss: 0.0103 - learning_rate: 2.5419e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 151ms/step - loss: 0.0138 - val_loss: 0.0103 - learning_rate: 2.2877e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 152ms/step - loss: 0.0134 - val_loss: 0.0104 - learning_rate: 2.0589e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - loss: 0.0134 - val_loss: 0.0104 - learning_rate: 1.8530e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 156ms/step - loss: 0.0136 - val_loss: 0.0104 - learning_rate: 1.6677e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 154ms/step - loss: 0.0133 - val_loss: 0.0102 - learning_rate: 1.5009e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 147ms/step - loss: 0.0135 - val_loss: 0.0102 - learning_rate: 1.3509e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 137ms/step - loss: 0.0133 - val_loss: 0.0103 - learning_rate: 1.2158e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 154ms/step - loss: 0.0132 - val_loss: 0.0103 - learning_rate: 1.0942e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 151ms/step - loss: 0.0132 - val_loss: 0.0102 - learning_rate: 9.8477e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 153ms/step - loss: 0.0134 - val_loss: 0.0101 - learning_rate: 8.8629e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 152ms/step - loss: 0.0134 - val_loss: 0.0102 - learning_rate: 7.9766e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 147ms/step - loss: 0.0137 - val_loss: 0.0103 - learning_rate: 7.1790e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 150ms/step - loss: 0.0132 - val_loss: 0.0102 - learning_rate: 6.4611e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 147ms/step - loss: 0.0139 - val_loss: 0.0102 - learning_rate: 5.8150e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 151ms/step - loss: 0.0137 - val_loss: 0.0102 - learning_rate: 5.2335e-05\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 200ms/step\n",
            "        Model        MAE          MSE       RMSE       MAPE  R2 Score\n",
            "0  VAE BiLSTM  14.040866  3335.316052  57.752195  63.695861  0.981331\n",
            "Normalized MSE VAE: 0.018668577669500967\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Predicted next 10 days (VAE): [[[  24.943432    59.492004     6.674197  1007.18225  ]\n",
            "  [  25.028173    59.511288     6.7114367 1017.4622   ]\n",
            "  [  25.027163    59.40445      6.714767  1028.8457   ]\n",
            "  [  25.006481    59.349335     6.723453  1030.0736   ]\n",
            "  [  24.964344    59.394394     6.761145  1027.7262   ]]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 1. Data Preparation\n",
        "df = pd.read_csv('DailyDelhiClimate.csv')\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True)  # Set 'date' as index\n",
        "\n",
        "# Features to use\n",
        "features = ['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
        "df = df[features]\n",
        "\n",
        "# Scaling\n",
        "scalers = {}\n",
        "for column in df.columns:\n",
        "    scaler = MinMaxScaler()\n",
        "    df[column] = scaler.fit_transform(df[[column]])\n",
        "    scalers['scaler_' + column] = scaler\n",
        "\n",
        "# Define n_past, n_future, and n_features\n",
        "n_past = 10  # Number of past time steps to use\n",
        "n_future = 5  # Number of future time steps to predict\n",
        "n_features = len(features)\n",
        "\n",
        "# Create sequences (sliding window approach)\n",
        "X = []\n",
        "y = []\n",
        "for i in range(n_past, len(df) - n_future + 1):\n",
        "    X.append(df.iloc[i - n_past:i].values)\n",
        "    y.append(df.iloc[i:i + n_future].values)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Train/Test Split (Temporal split - 80:20)\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# 2. VAE BiLSTM Encoder-Decoder Model\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
        "encoder_bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(encoder_inputs)\n",
        "encoder_bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(encoder_bilstm)\n",
        "\n",
        "# Variational layers\n",
        "z_mean = tf.keras.layers.Dense(16, name=\"z_mean\")(encoder_bilstm)\n",
        "z_log_var = tf.keras.layers.Dense(16, name=\"z_log_var\")(encoder_bilstm)\n",
        "\n",
        "# Define a custom layer for KL divergence calculation\n",
        "class KLDivergenceLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(KLDivergenceLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "        # Add the KL loss to the model's losses\n",
        "        self.add_loss(kl_loss)\n",
        "        return z_mean  # You can return z_mean or any other relevant output\n",
        "\n",
        "# Apply the custom layer\n",
        "z_mean = KLDivergenceLayer()([z_mean, z_log_var])\n",
        "\n",
        "# Reparameterization trick\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = tf.keras.layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = tf.keras.layers.RepeatVector(n_future)(z)\n",
        "decoder_bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(decoder_inputs)\n",
        "decoder_bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(decoder_bilstm)\n",
        "decoder_outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_bilstm)\n",
        "\n",
        "# VAE model\n",
        "vae = tf.keras.models.Model(encoder_inputs, decoder_outputs, name=\"vae_bilstm\")\n",
        "\n",
        "# 3. Compilation and Training\n",
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
        "history_vae = vae.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=16, verbose=1, callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "# 4. Prediction and Inverse Scaling\n",
        "pred_vae = vae.predict(X_test)\n",
        "\n",
        "for index, i in enumerate(features):  # Iterate through the feature names\n",
        "    scaler = scalers['scaler_' + i]\n",
        "    pred_vae[:, :, index] = scaler.inverse_transform(pred_vae[:, :, index])\n",
        "    y_train[:, :, index] = scaler.inverse_transform(y_train[:, :, index])\n",
        "    y_test[:, :, index] = scaler.inverse_transform(y_test[:, :, index])\n",
        "\n",
        "# 5. Evaluation (for the whole model across all 5 predicted days)\n",
        "\n",
        "# Define function for Mean Absolute Percentage Error (MAPE)\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Flatten the true and predicted values for overall comparison\n",
        "y_true_all = y_test.flatten()\n",
        "y_pred_vae_all = pred_vae.flatten()\n",
        "\n",
        "# Compute metrics for VAE model\n",
        "mae_vae = mean_absolute_error(y_true_all, y_pred_vae_all)\n",
        "mse_vae = mean_squared_error(y_true_all, y_pred_vae_all)\n",
        "rmse_vae = np.sqrt(mse_vae)\n",
        "mape_vae = mean_absolute_percentage_error(y_true_all, y_pred_vae_all)\n",
        "r2_vae = r2_score(y_true_all, y_pred_vae_all)\n",
        "\n",
        "# Display results in a structured table\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": [\"VAE BiLSTM\"],\n",
        "    \"MAE\": [mae_vae],\n",
        "    \"MSE\": [mse_vae],\n",
        "    \"RMSE\": [rmse_vae],\n",
        "    \"MAPE\": [mape_vae],\n",
        "    \"R2 Score\": [r2_vae]\n",
        "})\n",
        "\n",
        "print(results_df)\n",
        "\n",
        "# Reshape y_test and pred_vae to 2D\n",
        "y_test_2d = y_test.reshape(-1, y_test.shape[-1])\n",
        "pred_vae_2d = pred_vae.reshape(-1, pred_vae.shape[-1])\n",
        "\n",
        "# Normalize MSE using variance of actual data\n",
        "y_var = np.var(y_test_2d)\n",
        "normalized_mse_vae = mean_squared_error(y_test_2d, pred_vae_2d) / y_var\n",
        "\n",
        "print(\"Normalized MSE VAE:\", normalized_mse_vae)\n",
        "\n",
        "# Predict next 10 days\n",
        "X_test_last = X_test[-1:].reshape(1, n_past, n_features)\n",
        "pred_next_10_days_vae = vae.predict(X_test_last)\n",
        "\n",
        "# Inverse transform the predicted values\n",
        "for index, i in enumerate(features):\n",
        "    scaler = scalers['scaler_' + i]\n",
        "    pred_next_10_days_vae[:, :, index] = scaler.inverse_transform(pred_next_10_days_vae[:, :, index])\n",
        "\n",
        "print(\"Predicted next 10 days (VAE):\", pred_next_10_days_vae)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
